{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telecom Churn Prediction Project\n",
    "\n",
    "## Final Model Evaluation\n",
    "\n",
    "This notebook uses the model selected in the Modeling_final notebook and applies it to the test dataset to\n",
    "make a final assesment of the model.\n",
    "\n",
    "The model chosen from all the evaluation in the Modeling_final notebook is logistic regression.\n",
    "\n",
    "As described in the upstream notebooks, there is a strong correlation between the monthly contract customers and churn.  Given that the output of the model is to be used for targeted marketing the model will need to perform better than simply targeting marketing to the month to month customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "import umap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve,f1_score\n",
    "from sklearn.metrics import roc_auc_score, fbeta_score, confusion_matrix\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['customerID', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure',\n",
       "       'PhoneService', 'MultipleLines', 'OnlineSecurity', 'OnlineBackup',\n",
       "       'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n",
       "       'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges',\n",
       "       'Churn', 'Month-to-month', 'One year', 'DSL', 'Fiber optic', 'Female'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the csv file save by the clean/eda notebook\n",
    "train_df = pd.read_csv('../data/churn_train_clean.csv').drop('Unnamed: 0', axis=1)\n",
    "test_df = pd.read_csv('../data/churn_test_clean.csv').drop('Unnamed: 0', axis=1)\n",
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns that will be used for the model training and inputs.\n",
    "input_columns = ['SeniorCitizen', 'Partner', 'Dependents', \\\n",
    "       'tenure', 'PhoneService', 'MultipleLines',  \\\n",
    "       'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', \\\n",
    "       'StreamingTV', 'StreamingMovies', 'MonthlyCharges', 'TotalCharges', \\\n",
    "       'Month-to-month', 'One year', 'Fiber optic', 'Female']\n",
    "\n",
    "# Create the training test set\n",
    "X_train = train_df[input_columns]\n",
    "y_train = train_df['Churn']\n",
    "\n",
    "# Create the testing dataset\n",
    "X_test = test_df[input_columns]\n",
    "y_test = test_df['Churn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the training and test datasets have been defined let's build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8109028960817717\n"
     ]
    }
   ],
   "source": [
    "# Define the logistic regression model using the parameters output from the grid search in the upstream notebooks\n",
    "log_best = LogisticRegression(C=0.03, penalty='l2', solver='liblinear', random_state=45)\n",
    "\n",
    "# Fit the model using the training dataset\n",
    "log_best.fit(X_train,y_train)\n",
    "\n",
    "# Generate a first pass accuracy score\n",
    "log_y_hat = log_best.predict(X_test)\n",
    "log_y_proba = log_best.predict_proba(X_test)\n",
    "print(accuracy_score(y_test, log_y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8568659030286054"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, log_y_proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5448851774530271"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, log_y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6105263157894737"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, log_y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to look more closely at the model performance.  We will define a probability threshold and retest the model output applying that threshold.  From the new predictions we will generate a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the threshold we will use to generate a new prediction\n",
    "threshold = 0.5\n",
    "\n",
    "# Run the model and test against the threshold.\n",
    "y_predict = [1 if log_best.predict_proba(X_test)[x][1] > threshold else 0 for x in range(X_test.shape[0])]\n",
    "\n",
    "# Generate a confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix we can calculate Precision and Recall.  For this model we want to maximize Recall due to the relatively high 'cost' of losing a customer.  The formulas for precision and recall are:\n",
    "\n",
    "$$Precision = \\frac{True Positive}{True Positive + False Positive}$$\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$Recall = \\frac{True Positive}{True Positive + False Nagative}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Precision and Recall\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6941489361702128, 0.5448851774530271)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
